## snd spark submit

Runs the provided Beast V3 job with optional overrides

The overrides should be provided as a JSON file with the structure below.

If the 'clientTag' is not provided, a random tag will be generated.

If 'extraArguments', 'projectInputs', 'projectOutputs', or 'expectedParallelism' are not provided, the job will run with the default arguments.

<pre><code>
{
 "clientTag": "<string> (optional) - A tag for the client making the submission",
 "extraArguments": "<object> (optional) - Any additional arguments for the job",
 "projectInputs": [{
	"alias": "<string> (optional) - An alias for the input",
	"dataPath": "<string> (required) - The path to the input data",
	"dataFormat": "<string> (required) - The format of the input data"
	}
		// More input objects can be added here
	],
 "projectOutputs": [{
	"alias": "<string> (optional) - An alias for the output",
	"dataPath": "<string> (required) - The path where the output data should be stored",
	"dataFormat": "<string> (required) - The format of the output data"
	}
		// More output objects can be added here
	],
 "expectedParallelism": "<integer> (optional) - The expected level of parallelism for the job"
}
</code></pre>


```
snd spark submit [flags]
```

### Options

```
  -t, --client-tag string   Client tag for this submission
  -h, --help                help for submit
  -n, --job-name string     Beast SparkJob or SparkJobReference resource name
  -o, --overrides string    Overrides for the provided job name
```

### Options inherited from parent commands

```
  -a, --auth-provider string        Specify the OAuth provider name (default "azuread")
      --custom-service-url string   Specify the service url (default "https://beast-v3.%s.sneaksanddata.com")
  -e, --env string                  Target environment (default "test")
  -i, --id string                   Specify the  Job ID
```

### SEE ALSO

* [snd spark](snd_spark.md)	 - Manage Spark jobs

###### Auto generated by spf13/cobra on 29-Apr-2024
